{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import coloredlogs\n",
    "import logging\n",
    "\n",
    "import gym\n",
    "\n",
    "import maddpg.common.tf_util as U\n",
    "from maddpg.trainer.maddpg import MADDPGAgentTrainer\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "coloredlogs.install()\n",
    "\n",
    "class Params(object):\n",
    "    def __init__(self):\n",
    "        # Environment\n",
    "        self.scenario = \"simple_spread_sparse\"\n",
    "        self.max_episode_len = 25\n",
    "        self.num_episodes = 60000\n",
    "        self.num_adversaries = 0\n",
    "        self.good_policy = \"maddpg\"\n",
    "        self.adv_policy = \"maddpg\"\n",
    "        # Core training parameters\n",
    "        self.lr = 1e-2\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 1024\n",
    "        self.num_units = 64\n",
    "        # Checkpointing\n",
    "        self.exp_name = None\n",
    "        self.save_dir = \"/tmp/policy\"\n",
    "        self.save_rate = 1000\n",
    "        self.load_dir = \"\"\n",
    "        # Evaluation\n",
    "        self.restore = False\n",
    "        self.display = False\n",
    "        self.benchmark = False\n",
    "        self.benchmark_iters = 100000\n",
    "        self.benchmark_dir = \"./benchmark_files/\"\n",
    "        self.plots_dir = \"./learning_curves/\"\n",
    "\n",
    "def mlp_model(input, num_outputs, scope, reuse=False, num_units=64, rnn_cell=None):\n",
    "    # This model takes as input an observation and returns values of all actions\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = input\n",
    "        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n",
    "        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n",
    "        out = layers.fully_connected(out, num_outputs=num_outputs, activation_fn=None)\n",
    "        return out\n",
    "\n",
    "def make_env(scenario_name, arglist, benchmark=False):\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "    import multiagent.scenarios as scenarios\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    if benchmark:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
    "    else:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
    "    return env\n",
    "\n",
    "def get_trainers(env, num_adversaries, obs_shape_n, arglist):\n",
    "    trainers = []\n",
    "    model = mlp_model\n",
    "    trainer = MADDPGAgentTrainer\n",
    "    for i in range(num_adversaries):\n",
    "        trainers.append(trainer(\n",
    "            \"agent_%d\" % i, model, obs_shape_n, env.action_space, i, arglist,\n",
    "            local_q_func=(arglist.adv_policy=='ddpg')))\n",
    "    for i in range(num_adversaries, env.n):\n",
    "        trainers.append(trainer(\n",
    "            \"agent_%d\" % i, model, obs_shape_n, env.action_space, i, arglist,\n",
    "            local_q_func=(arglist.good_policy=='ddpg')))\n",
    "    return trainers\n",
    "\n",
    "\n",
    "def train(arglist):\n",
    "    with U.single_threaded_session():\n",
    "        # Create environment\n",
    "        env = make_env(arglist.scenario, arglist, arglist.benchmark)\n",
    "        if not isinstance(env.action_space[0], gym.spaces.discrete.Discrete):\n",
    "            logging.warning(\"Action space is not Discrete object. Check the discrete_action_space flag in multiagent/environment.py\")\n",
    "        # Create agent trainers\n",
    "        obs_shape_n = [env.observation_space[i].shape for i in range(env.n)]\n",
    "        num_adversaries = min(env.n, arglist.num_adversaries)\n",
    "        trainers = get_trainers(env, num_adversaries, obs_shape_n, arglist)\n",
    "        print('Using good policy {} and adv policy {}'.format(arglist.good_policy, arglist.adv_policy))\n",
    "\n",
    "        # Initialize\n",
    "        U.initialize()\n",
    "\n",
    "        # Load previous results, if necessary\n",
    "        if arglist.load_dir == \"\":\n",
    "            arglist.load_dir = arglist.save_dir\n",
    "        if arglist.display or arglist.restore or arglist.benchmark:\n",
    "            print('Loading previous state...')\n",
    "            U.load_state(arglist.load_dir)\n",
    "\n",
    "        episode_rewards = [0.0]  # sum of rewards for all agents\n",
    "        agent_rewards = [[0.0] for _ in range(env.n)]  # individual agent reward\n",
    "        final_ep_rewards = []  # sum of rewards for training curve\n",
    "        final_ep_ag_rewards = []  # agent rewards for training curve\n",
    "        agent_info = [[[]]]  # placeholder for benchmarking info\n",
    "        saver = tf.train.Saver()\n",
    "        obs_n = env.reset()\n",
    "        episode_step = 0\n",
    "        train_step = 0\n",
    "        t_start = time.time()\n",
    "\n",
    "        print('Starting iterations...')\n",
    "        while True:\n",
    "            # get action\n",
    "            action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]\n",
    "            # environment step\n",
    "            new_obs_n, rew_n, done_n, info_n = env.step(action_n)\n",
    "            episode_step += 1\n",
    "            done = all(done_n)\n",
    "            terminal = (episode_step >= arglist.max_episode_len)\n",
    "            # collect experience\n",
    "            for i, agent in enumerate(trainers):\n",
    "                agent.experience(obs_n[i], action_n[i], rew_n[i], new_obs_n[i], done_n[i], terminal)\n",
    "            obs_n = new_obs_n\n",
    "\n",
    "            for i, rew in enumerate(rew_n):\n",
    "                episode_rewards[-1] += rew\n",
    "                agent_rewards[i][-1] += rew\n",
    "\n",
    "            if done or terminal:\n",
    "                obs_n = env.reset()\n",
    "                episode_step = 0\n",
    "                episode_rewards.append(0)\n",
    "                for a in agent_rewards:\n",
    "                    a.append(0)\n",
    "                agent_info.append([[]])\n",
    "\n",
    "            # increment global step counter\n",
    "            train_step += 1\n",
    "\n",
    "            # for benchmarking learned policies\n",
    "            if arglist.benchmark:\n",
    "                for i, info in enumerate(info_n):\n",
    "                    agent_info[-1][i].append(info_n['n'])\n",
    "                if train_step > arglist.benchmark_iters and (done or terminal):\n",
    "                    file_name = arglist.benchmark_dir + arglist.exp_name + '.pkl'\n",
    "                    print('Finished benchmarking, now saving...')\n",
    "                    with open(file_name, 'wb') as fp:\n",
    "                        pickle.dump(agent_info[:-1], fp)\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # for displaying learned policies\n",
    "            if arglist.display:\n",
    "                time.sleep(0.1)\n",
    "                env.render()\n",
    "                continue\n",
    "\n",
    "            # update all trainers, if not in display or benchmark mode\n",
    "            loss = None\n",
    "            for agent in trainers:\n",
    "                agent.preupdate()\n",
    "            for agent in trainers:\n",
    "                loss = agent.update(trainers, train_step)\n",
    "\n",
    "            # save model, display training output\n",
    "            if terminal and (len(episode_rewards) % arglist.save_rate == 0):\n",
    "                U.save_state(arglist.save_dir, saver=saver)\n",
    "                # print statement depends on whether or not there are adversaries\n",
    "                if num_adversaries == 0:\n",
    "                    print(\"steps: {}, episodes: {}, mean episode reward: {}, time: {}\".format(\n",
    "                        train_step, len(episode_rewards), np.mean(episode_rewards[-arglist.save_rate:]), round(time.time()-t_start, 3)))\n",
    "                else:\n",
    "                    print(\"steps: {}, episodes: {}, mean episode reward: {}, agent episode reward: {}, time: {}\".format(\n",
    "                        train_step, len(episode_rewards), np.mean(episode_rewards[-arglist.save_rate:]),\n",
    "                        [np.mean(rew[-arglist.save_rate:]) for rew in agent_rewards], round(time.time()-t_start, 3)))\n",
    "                t_start = time.time()\n",
    "                # Keep track of final episode reward\n",
    "                final_ep_rewards.append(np.mean(episode_rewards[-arglist.save_rate:]))\n",
    "                for rew in agent_rewards:\n",
    "                    final_ep_ag_rewards.append(np.mean(rew[-arglist.save_rate:]))\n",
    "\n",
    "            # saves final episode reward for plotting training curve later\n",
    "            if len(episode_rewards) > arglist.num_episodes:\n",
    "                rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'\n",
    "                with open(rew_file_name, 'wb') as fp:\n",
    "                    pickle.dump(final_ep_rewards, fp)\n",
    "                agrew_file_name = arglist.plots_dir + arglist.exp_name + '_agrewards.pkl'\n",
    "                with open(agrew_file_name, 'wb') as fp:\n",
    "                    pickle.dump(final_ep_ag_rewards, fp)\n",
    "                print('...Finished total of {} episodes.'.format(len(episode_rewards)))\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = Params()\n",
    "    train(params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
