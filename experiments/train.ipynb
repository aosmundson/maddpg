{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using good policy maddpg and adv policy maddpg\n",
      "Loading previous state...\n",
      "INFO:tensorflow:Restoring parameters from /tmp/policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-06 23:13:46 ip-172-31-12-102 tensorflow[6841] INFO Restoring parameters from /tmp/policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterations...\n",
      "\n"
     ]
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9df5424432b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-9df5424432b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(arglist)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marglist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multiagent-particle-envs/multiagent/environment.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# import rendering only if we need it (and don't import for headless machines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;31m#from gym.envs.classic_control import rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 \u001b[0;32mfrom\u001b[0m \u001b[0mmultiagent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multiagent-particle-envs/multiagent/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1896\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \"\"\"\n\u001b[0;32m-> 1845\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_pyglet_docgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import coloredlogs\n",
    "import logging\n",
    "\n",
    "import gym\n",
    "\n",
    "import maddpg.common.tf_util as U\n",
    "from maddpg.trainer.maddpg import MADDPGAgentTrainer\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "coloredlogs.install()\n",
    "\n",
    "class Params(object):\n",
    "    def __init__(self):\n",
    "        # Environment\n",
    "        self.scenario = \"simple_spread_sparse\"\n",
    "        self.max_episode_len = 25\n",
    "        self.num_episodes = 60000\n",
    "        self.num_adversaries = 0\n",
    "        self.good_policy = \"maddpg\"\n",
    "        self.adv_policy = \"maddpg\"\n",
    "        # Core training parameters\n",
    "        self.lr = 1e-2\n",
    "        self.gamma = 0.95\n",
    "        self.batch_size = 1024\n",
    "        self.num_units = 64\n",
    "        # Checkpointing\n",
    "        self.exp_name = None\n",
    "        self.save_dir = \"/tmp/policy\"\n",
    "        self.save_rate = 1000\n",
    "        self.load_dir = \"\"\n",
    "        # Evaluation\n",
    "        self.restore = False\n",
    "        self.display = False\n",
    "        self.benchmark = False\n",
    "        self.benchmark_iters = 100000\n",
    "        self.benchmark_dir = \"./benchmark_files/\"\n",
    "        self.plots_dir = \"./learning_curves/\"\n",
    "\n",
    "def mlp_model(input, num_outputs, scope, reuse=False, num_units=64, rnn_cell=None):\n",
    "    # This model takes as input an observation and returns values of all actions\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = input\n",
    "        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n",
    "        out = layers.fully_connected(out, num_outputs=num_units, activation_fn=tf.nn.relu)\n",
    "        out = layers.fully_connected(out, num_outputs=num_outputs, activation_fn=None)\n",
    "        return out\n",
    "\n",
    "def make_env(scenario_name, arglist, benchmark=False):\n",
    "    from multiagent.environment import MultiAgentEnv\n",
    "    import multiagent.scenarios as scenarios\n",
    "\n",
    "    # load scenario from script\n",
    "    scenario = scenarios.load(scenario_name + \".py\").Scenario()\n",
    "    # create world\n",
    "    world = scenario.make_world()\n",
    "    # create multiagent environment\n",
    "    if benchmark:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation, scenario.benchmark_data)\n",
    "    else:\n",
    "        env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)\n",
    "    return env\n",
    "\n",
    "def get_trainers(env, num_adversaries, obs_shape_n, arglist):\n",
    "    trainers = []\n",
    "    model = mlp_model\n",
    "    trainer = MADDPGAgentTrainer\n",
    "    for i in range(num_adversaries):\n",
    "        trainers.append(trainer(\n",
    "            \"agent_%d\" % i, model, obs_shape_n, env.action_space, i, arglist,\n",
    "            local_q_func=(arglist.adv_policy=='ddpg')))\n",
    "    for i in range(num_adversaries, env.n):\n",
    "        trainers.append(trainer(\n",
    "            \"agent_%d\" % i, model, obs_shape_n, env.action_space, i, arglist,\n",
    "            local_q_func=(arglist.good_policy=='ddpg')))\n",
    "    return trainers\n",
    "\n",
    "\n",
    "def train(arglist):\n",
    "    with U.single_threaded_session():\n",
    "        # Create environment\n",
    "        env = make_env(arglist.scenario, arglist, arglist.benchmark)\n",
    "        if not isinstance(env.action_space[0], gym.spaces.discrete.Discrete):\n",
    "            logging.warning(\"Action space is not Discrete object. Check the discrete_action_space flag in multiagent/environment.py\")\n",
    "        # Create agent trainers\n",
    "        obs_shape_n = [env.observation_space[i].shape for i in range(env.n)]\n",
    "        num_adversaries = min(env.n, arglist.num_adversaries)\n",
    "        trainers = get_trainers(env, num_adversaries, obs_shape_n, arglist)\n",
    "        print('Using good policy {} and adv policy {}'.format(arglist.good_policy, arglist.adv_policy))\n",
    "\n",
    "        # Initialize\n",
    "        U.initialize()\n",
    "\n",
    "        # Load previous results, if necessary\n",
    "        if arglist.load_dir == \"\":\n",
    "            arglist.load_dir = arglist.save_dir\n",
    "        if arglist.display or arglist.restore or arglist.benchmark:\n",
    "            print('Loading previous state...')\n",
    "            U.load_state(arglist.load_dir)\n",
    "\n",
    "        episode_rewards = [0.0]  # sum of rewards for all agents\n",
    "        agent_rewards = [[0.0] for _ in range(env.n)]  # individual agent reward\n",
    "        final_ep_rewards = []  # sum of rewards for training curve\n",
    "        final_ep_ag_rewards = []  # agent rewards for training curve\n",
    "        agent_info = [[[]]]  # placeholder for benchmarking info\n",
    "        saver = tf.train.Saver()\n",
    "        obs_n = env.reset()\n",
    "        episode_step = 0\n",
    "        train_step = 0\n",
    "        t_start = time.time()\n",
    "\n",
    "        print('Starting iterations...')\n",
    "        while True:\n",
    "            # get action\n",
    "            action_n = [agent.action(obs) for agent, obs in zip(trainers,obs_n)]\n",
    "            # environment step\n",
    "            new_obs_n, rew_n, done_n, info_n = env.step(action_n)\n",
    "            episode_step += 1\n",
    "            done = all(done_n)\n",
    "            terminal = (episode_step >= arglist.max_episode_len)\n",
    "            # collect experience\n",
    "            for i, agent in enumerate(trainers):\n",
    "                agent.experience(obs_n[i], action_n[i], rew_n[i], new_obs_n[i], done_n[i], terminal)\n",
    "            obs_n = new_obs_n\n",
    "\n",
    "            for i, rew in enumerate(rew_n):\n",
    "                episode_rewards[-1] += rew\n",
    "                agent_rewards[i][-1] += rew\n",
    "\n",
    "            if done or terminal:\n",
    "                obs_n = env.reset()\n",
    "                episode_step = 0\n",
    "                episode_rewards.append(0)\n",
    "                for a in agent_rewards:\n",
    "                    a.append(0)\n",
    "                agent_info.append([[]])\n",
    "\n",
    "            # increment global step counter\n",
    "            train_step += 1\n",
    "\n",
    "            # for benchmarking learned policies\n",
    "            if arglist.benchmark:\n",
    "                for i, info in enumerate(info_n):\n",
    "                    agent_info[-1][i].append(info_n['n'])\n",
    "                if train_step > arglist.benchmark_iters and (done or terminal):\n",
    "                    file_name = arglist.benchmark_dir + arglist.exp_name + '.pkl'\n",
    "                    print('Finished benchmarking, now saving...')\n",
    "                    with open(file_name, 'wb') as fp:\n",
    "                        pickle.dump(agent_info[:-1], fp)\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "            # for displaying learned policies\n",
    "            if arglist.display:\n",
    "                time.sleep(0.1)\n",
    "                env.render()\n",
    "                continue\n",
    "\n",
    "            # update all trainers, if not in display or benchmark mode\n",
    "            loss = None\n",
    "            for agent in trainers:\n",
    "                agent.preupdate()\n",
    "            for agent in trainers:\n",
    "                loss = agent.update(trainers, train_step)\n",
    "\n",
    "            # save model, display training output\n",
    "            if terminal and (len(episode_rewards) % arglist.save_rate == 0):\n",
    "                U.save_state(arglist.save_dir, saver=saver)\n",
    "                # print statement depends on whether or not there are adversaries\n",
    "                if num_adversaries == 0:\n",
    "                    print(\"steps: {}, episodes: {}, mean episode reward: {}, time: {}\".format(\n",
    "                        train_step, len(episode_rewards), np.mean(episode_rewards[-arglist.save_rate:]), round(time.time()-t_start, 3)))\n",
    "                else:\n",
    "                    print(\"steps: {}, episodes: {}, mean episode reward: {}, agent episode reward: {}, time: {}\".format(\n",
    "                        train_step, len(episode_rewards), np.mean(episode_rewards[-arglist.save_rate:]),\n",
    "                        [np.mean(rew[-arglist.save_rate:]) for rew in agent_rewards], round(time.time()-t_start, 3)))\n",
    "                t_start = time.time()\n",
    "                # Keep track of final episode reward\n",
    "                final_ep_rewards.append(np.mean(episode_rewards[-arglist.save_rate:]))\n",
    "                for rew in agent_rewards:\n",
    "                    final_ep_ag_rewards.append(np.mean(rew[-arglist.save_rate:]))\n",
    "\n",
    "            # saves final episode reward for plotting training curve later\n",
    "            if len(episode_rewards) > arglist.num_episodes:\n",
    "                rew_file_name = arglist.plots_dir + arglist.exp_name + '_rewards.pkl'\n",
    "                with open(rew_file_name, 'wb') as fp:\n",
    "                    pickle.dump(final_ep_rewards, fp)\n",
    "                agrew_file_name = arglist.plots_dir + arglist.exp_name + '_agrewards.pkl'\n",
    "                with open(agrew_file_name, 'wb') as fp:\n",
    "                    pickle.dump(final_ep_ag_rewards, fp)\n",
    "                print('...Finished total of {} episodes.'.format(len(episode_rewards)))\n",
    "                break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    params = Params()\n",
    "    train(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
